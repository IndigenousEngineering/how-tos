{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook is intended to be a reference for using scikit learn in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neccessary imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give a 5x3 matrix with random integers from 0 to 199\n",
    "\n",
    "dataset = np.random.randint(0,200,(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[113, 155,  27],\n",
       "       [ 24, 161, 188],\n",
       "       [ 90, 178, 138],\n",
       "       [ 53,  98,  98],\n",
       "       [ 83, 102, 116]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalizing: MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.preprocessing.data.MinMaxScaler"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale data: instatiate a MinMaxScaler object\n",
    "my_scaler = MinMaxScaler()\n",
    "\n",
    "type(my_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/susanna/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit to the data\n",
    "# scikit-learn follows conventions like data.fit()\n",
    "\n",
    "my_scaler.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.7125    , 0.        ],\n",
       "       [0.        , 0.7875    , 1.        ],\n",
       "       [0.74157303, 1.        , 0.68944099],\n",
       "       [0.3258427 , 0.        , 0.44099379],\n",
       "       [0.66292135, 0.05      , 0.55279503]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the data\n",
    "\n",
    "my_scaler.transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data are transformed such that the minimum value becomes zero (0) and the max value becomes one (1).\n",
    "#### * there are other ways to normalize data, but this one is pretty solid for many machine learning purposes\n",
    "#### * this can actually be done in one command, using the MinMaxScaler fit_transform() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/susanna/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.7125    , 0.        ],\n",
       "       [0.        , 0.7875    , 1.        ],\n",
       "       [0.74157303, 1.        , 0.68944099],\n",
       "       [0.3258427 , 0.        , 0.44099379],\n",
       "       [0.66292135, 0.05      , 0.55279503]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one line, same results using fit_transform() method\n",
    "\n",
    "my_scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this one-step fit and transform works great here when we're using the entire dataset. BUT when we're constructing a model, we'd typically want to use a train-test-split for model validation, meaning\n",
    "#### * we would've split the data up into training and testing sets\n",
    "#### * we would want to keep the test dataset a secret from our model\n",
    "#### * so fitting to the entire dataset then basing the normalization off of that fit is cheating! it's giving the model a sneak-peek at (or clues to) the data\n",
    "### * in real life, it's better to fit to the TRAINING data, THEN transform the training and testing data seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-test-split is your friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data using pandas dataframe\n",
    "# this one gives a 50x5 dataframe of random integers from 0 to 200\n",
    "# four columns of features plus one for labels\n",
    "\n",
    "new_data = pd.DataFrame(data=np.random.randint(0,201,(50,5)), \n",
    "                        columns=['feature_1', 'feature_2', 'feature_3', 'feature_4', \n",
    "                                 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels from data\n",
    "# .drop() axes: for rows: use 0; for columns: use 1\n",
    "# alternatively just set X = ['feature_1', 'feature_2',...] etc\n",
    "\n",
    "X = new_data.drop('label', axis=1)\n",
    "\n",
    "y = new_data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn has a pretty cool model_selection suite! train_test_split is part of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train_test_split to futher split up the data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 4) (10, 4) (40,) (10,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### that's it (:\n",
    "\n",
    "for more information check out scikit-learn model selection documentation at http://scikit-learn.org/stable/model_selection.html#model-selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
